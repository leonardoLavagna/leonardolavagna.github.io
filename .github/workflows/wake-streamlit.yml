name: Wake Streamlit apps (from _data/applets.yml)

on:
  schedule:
    - cron: "*/20 * * * *"   # every 20 minutes (UTC)
  workflow_dispatch: {}       # allow manual runs

concurrency:
  group: wake-streamlit
  cancel-in-progress: true

jobs:
  ping:
    runs-on: ubuntu-latest
    timeout-minutes: 8

    steps:
      - uses: actions/checkout@v4

      - name: Extract app URLs
        run: |
          if [ ! -f "_data/applets.yml" ]; then
            echo "::error::_data/applets.yml not found"
            exit 1
          fi

          # 1) Grab `url:` lines
          # 2) Drop the leading 'url: '
          # 3) Strip ?embed=true / &embed=true
          # 4) Remove surrounding double quotes
          # 5) Trim trailing whitespace
          grep -E '^\s*url:\s*' _data/applets.yml \
            | sed -E 's/^\s*url:\s*//' \
            | sed -E 's/[?&]embed=true//g' \
            | sed -E 's/^"(.*)"$/\1/' \
            | sed -E 's/\s+$//' \
            > urls.txt

          echo "Found URLs:"
          cat urls.txt

      - name: Wake each app (robust)
        run: |
          if [ ! -s urls.txt ]; then
            echo "::warning::No URLs to ping."
            exit 0
          fi

          while IFS= read -r url; do
            [ -z "$url" ] && continue

            # quick DNS sanity check
            host=$(printf "%s" "$url" | awk -F/ '{print $3}')
            if ! getent hosts "$host" >/dev/null 2>&1; then
              echo "::warning::Skipping $url â€” host not resolvable"
              continue
            fi

            echo "Waking: $url"
            # follow redirects (cap at 10), retry a bit for cold starts
            http_and_effective=$(curl -sS -L -o /dev/null \
              -w "code=%{http_code} effective=%{url_effective}" \
              -A "keepalive-bot/1.1 (+github actions)" \
              --connect-timeout 20 --max-time 120 --max-redirs 10 \
              --retry 2 --retry-delay 10 "$url")
            curl_status=$?

            echo "curl_exit=$curl_status $http_and_effective"

            # don't fail whole job if one URL misbehaves
            if [ $curl_status -ne 0 ]; then
              echo "::warning::curl failed for $url (exit $curl_status)"
              continue
            fi

            code=$(printf "%s" "$http_and_effective" | sed -n 's/.*code=\([0-9][0-9][0-9]\).*/\1/p')
            # Treat <500 as OK; cold starts can return 200/302/403 while spinning up
            if [ -z "$code" ] || [ "$code" -ge 500 ]; then
              echo "::warning::Wake likely failed for $url (HTTP ${code:-unknown})"
            fi
          done < urls.txt

          exit 0
